{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6228bc1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6228bc1c",
        "outputId": "fb845e61-cc5e-4839-9774-9f56dc526421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (6.1.1)\n",
            "Requirement already satisfied: pyparsing in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (2024.12.14)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bdWYrgjeZerj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdWYrgjeZerj",
        "outputId": "a1df1bdc-ad74-410f-9378-058c587a5db8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"from google.colab import drive\n",
        "drive.mount('/content/drive')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "817b1078",
      "metadata": {
        "id": "817b1078"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "# PyG\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "# Vostri moduli\n",
        "from src.loadData    import (GraphDataset, TestDataset)\n",
        "from src.models      import GNN\n",
        "from src.transforms import EdgeDropout, NodeDropout, Compose\n",
        "from src.losses import (\n",
        "    NoisyCrossEntropyLoss,\n",
        "    GeneralizedCELoss,\n",
        "    SymmetricCELoss,\n",
        "    estimate_transition_matrix,\n",
        "    ForwardCorrectionLoss,\n",
        "    BootstrappingLoss\n",
        ")\n",
        "from src.divide_mix_def import DivideMixTrainer\n",
        "from src.utils import set_seed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0a9c70d7",
      "metadata": {
        "id": "0a9c70d7"
      },
      "outputs": [],
      "source": [
        "def add_zeros(data):\n",
        "    if not hasattr(data, 'x') or data.x is None:\n",
        "        data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3622cfa1",
      "metadata": {
        "id": "3622cfa1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\\n    \\n    model = model.to(device)\\n    model.train()\\n    total_loss = 0\\n    correct = 0\\n    total = 0\\n    \\n    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\\n        data = data.to(device)\\n        optimizer.zero_grad()\\n        output = model(data)\\n        loss = criterion(output, data.y)\\n        loss.backward()\\n        optimizer.step()\\n        total_loss += loss.item()\\n        pred = output.argmax(dim=1)\\n        correct += (pred == data.y).sum().item()\\n        total += data.y.size(0)\\n\\n    # Save checkpoints if required\\n    if save_checkpoints:\\n        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\\n        torch.save(model.state_dict(), checkpoint_file)\\n        print(f\"Checkpoint saved at {checkpoint_file}\")\\n\\n    print(f\"Train loss/acc: {total_loss / len(data_loader):.4f}/{correct / total:.4f}\")\\n    return total_loss / len(data_loader),  correct / total'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
        "    \n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "        total += data.y.size(0)\n",
        "\n",
        "    # Save checkpoints if required\n",
        "    if save_checkpoints:\n",
        "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
        "        torch.save(model.state_dict(), checkpoint_file)\n",
        "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
        "\n",
        "    print(f\"Train loss/acc: {total_loss / len(data_loader):.4f}/{correct / total:.4f}\")\n",
        "    return total_loss / len(data_loader),  correct / total\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46870e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(model, val_loader, device):\n",
        "    \"\"\"Validate model performance on clean validation set.\"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            outputs = model(batch)\n",
        "            preds = outputs.argmax(1)\n",
        "            y_pred.extend(preds.cpu().tolist())\n",
        "            y_true.extend(batch.y.cpu().tolist())\n",
        "    \n",
        "    acc = sum(p == t for p, t in zip(y_pred, y_true)) / len(y_true)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    criterion,\n",
        "    optimizer, \n",
        "    device,\n",
        "    num_epochs=100,\n",
        "    patience=10,\n",
        "    batch_size=32\n",
        "):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    best_val_f1 = 0\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        y_true_train, y_pred_train = [], []\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            batch = batch.to(device)\n",
        "            labels = batch.y\n",
        "\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            y_pred_train.extend(preds.cpu().tolist())\n",
        "            y_true_train.extend(labels.cpu().tolist())\n",
        "\n",
        "        # Compute training metrics\n",
        "        train_acc = sum(p == t for p, t in zip(y_pred_train, y_true_train)) / len(y_true_train)\n",
        "        train_f1 = f1_score(y_true_train, y_pred_train, average='macro')\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        val_acc, val_f1 = validate(model, val_loader, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: \"\n",
        "              f\"Train Loss={avg_train_loss:.4f}, \"\n",
        "              f\"Train Acc={train_acc:.4f}, \"\n",
        "              f\"Train F1={train_f1:.4f}, \"\n",
        "              f\"Val Acc={val_acc:.4f}, \"\n",
        "              f\"Val F1={val_f1:.4f} (Best={best_val_f1:.4f})\")\n",
        "\n",
        "        # Early stopping based on F1\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_model_state = deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    # Restore best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6139b912",
      "metadata": {
        "id": "6139b912"
      },
      "outputs": [],
      "source": [
        "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions = []\n",
        "    total_loss = 0\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "\n",
        "            if calculate_accuracy:\n",
        "                correct += (pred == data.y).sum().item()\n",
        "                total += data.y.size(0)\n",
        "                total_loss += criterion(output, data.y).item()\n",
        "            else:\n",
        "                predictions.extend(pred.cpu().numpy())\n",
        "\n",
        "    \n",
        "    if calculate_accuracy:\n",
        "        accuracy = correct / total\n",
        "        return  total_loss / len(data_loader),accuracy\n",
        "        print(f\"Test loss/acc {total_loss / len(data_loader):.4f} / {correct / total:.4f}\")\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fbdbd871",
      "metadata": {
        "id": "fbdbd871"
      },
      "outputs": [],
      "source": [
        "def save_predictions(predictions, test_path):\n",
        "    script_dir = os.getcwd()\n",
        "    submission_folder = os.path.join(script_dir, \"submission\")\n",
        "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
        "\n",
        "    os.makedirs(submission_folder, exist_ok=True)\n",
        "\n",
        "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
        "\n",
        "    test_graph_ids = list(range(len(predictions)))\n",
        "    output_df = pd.DataFrame({\n",
        "        \"id\": test_graph_ids,\n",
        "        \"pred\": predictions\n",
        "    })\n",
        "\n",
        "    output_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Predictions saved to {output_csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fc3d24da",
      "metadata": {
        "id": "fc3d24da"
      },
      "outputs": [],
      "source": [
        "def plot_training_progress(train_losses, train_accuracies, save_plot, output_dir):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss per Epoch')\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training Accuracy per Epoch')\n",
        "\n",
        "    if(save_plot):\n",
        "        # Save plots in the current directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8cMtQXyuWQol",
      "metadata": {
        "id": "8cMtQXyuWQol"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "device                =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Modello\n",
        "gnn_type              = 'gcn-virtual'   \n",
        "num_layer             = 2\n",
        "emb_dim               = 300\n",
        "drop_ratio            = 0.8\n",
        "\n",
        "pooling               = \"attention\"\n",
        "\n",
        "edge_p  = 0.5   # frazione di bordi da droppare\n",
        "node_p = 0.5\n",
        "\n",
        "lr                    = 0.01\n",
        "epochs                = 100\n",
        "weight_decay          = 0\n",
        "num_classes           = 6\n",
        "batch_size            = 32 \n",
        "patience              = 12\n",
        "nesterov              = True\n",
        "momentum              = 0.9\n",
        "residual              = True\n",
        "seed                  = set_seed(42)\n",
        "transforms = Compose([\n",
        "    EdgeDropout(p=edge_p),\n",
        "    NodeDropout(p=node_p),\n",
        "    add_zeros,\n",
        "])\n",
        "\n",
        "# Epoch 49: Train Loss=0.3052, Train Acc=0.9117, Val Acc=0.6953 DATASET C"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41679bf6",
      "metadata": {},
      "source": [
        "# Minimum Effort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "502f55df",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_gnn_model(gnn_type, num_classes, num_layer, emb_dim, drop_ratio, device, residual, pooling):\n",
        "    kwargs = {\n",
        "        'gnn_type': gnn_type.replace(\"-virtual\", \"\"),\n",
        "        'num_class': num_classes,\n",
        "        'num_layer': num_layer,\n",
        "        'emb_dim': emb_dim,\n",
        "        'drop_ratio': drop_ratio,\n",
        "        'virtual_node': \"virtual\" in gnn_type,\n",
        "        'residual': residual,\n",
        "        'graph_pooling': pooling\n",
        "        }\n",
        "\n",
        "    model = GNN(**kwargs).to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b7538579",
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_dataset(dataset: GraphDataset, val_ratio=0.1, seed=42):\n",
        "    labels = torch.tensor([data.y.item() for data in dataset])\n",
        "    indices = list(range(len(dataset)))\n",
        "\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=val_ratio,\n",
        "        stratify=labels,\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    train_subset = []\n",
        "    val_subset = []\n",
        "\n",
        "    for new_idx, original_idx in enumerate(train_idx):\n",
        "        data = dataset[original_idx]\n",
        "        data.idx = new_idx  # Normalize index\n",
        "        train_subset.append(data)\n",
        "\n",
        "    for new_idx, original_idx in enumerate(val_idx):\n",
        "        data = dataset[original_idx]\n",
        "        data.idx = new_idx  # Normalize index\n",
        "        val_subset.append(data)\n",
        "\n",
        "    return train_subset, val_subset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4696ca96",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating dataset\n",
            "Splitting dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100: 100%|██████████| 140/140 [00:00<00:00, 170.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss=2.2321, Train Acc=0.2013, Train F1=0.1744, Val Acc=0.1732, Val F1=0.0569 (Best=0.0000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/100: 100%|██████████| 140/140 [00:00<00:00, 213.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss=1.9359, Train Acc=0.2098, Train F1=0.1678, Val Acc=0.2330, Val F1=0.1040 (Best=0.0569)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/100: 100%|██████████| 140/140 [00:00<00:00, 204.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss=1.8219, Train Acc=0.2342, Train F1=0.1589, Val Acc=0.2509, Val F1=0.0767 (Best=0.1040)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/100: 100%|██████████| 140/140 [00:00<00:00, 208.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Train Loss=1.7754, Train Acc=0.2554, Train F1=0.1603, Val Acc=0.2687, Val F1=0.1173 (Best=0.1040)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/100: 100%|██████████| 140/140 [00:00<00:00, 209.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Train Loss=1.7556, Train Acc=0.2712, Train F1=0.1673, Val Acc=0.3187, Val F1=0.1957 (Best=0.1173)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/100: 100%|██████████| 140/140 [00:00<00:00, 202.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Train Loss=1.7435, Train Acc=0.2766, Train F1=0.1684, Val Acc=0.2991, Val F1=0.1395 (Best=0.1957)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/100: 100%|██████████| 140/140 [00:00<00:00, 207.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Train Loss=1.7327, Train Acc=0.2893, Train F1=0.1867, Val Acc=0.3080, Val F1=0.1611 (Best=0.1957)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/100: 100%|██████████| 140/140 [00:00<00:00, 211.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Train Loss=1.7254, Train Acc=0.2940, Train F1=0.1850, Val Acc=0.2759, Val F1=0.1885 (Best=0.1957)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/100: 100%|██████████| 140/140 [00:00<00:00, 209.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Train Loss=1.7281, Train Acc=0.2955, Train F1=0.1891, Val Acc=0.2714, Val F1=0.1209 (Best=0.1957)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/100: 100%|██████████| 140/140 [00:00<00:00, 207.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Train Loss=1.7262, Train Acc=0.2969, Train F1=0.1939, Val Acc=0.2062, Val F1=0.0997 (Best=0.1957)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/100: 100%|██████████| 140/140 [00:00<00:00, 211.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Train Loss=1.7229, Train Acc=0.3018, Train F1=0.1926, Val Acc=0.3321, Val F1=0.2062 (Best=0.1957)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/100: 100%|██████████| 140/140 [00:00<00:00, 207.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Train Loss=1.7136, Train Acc=0.3069, Train F1=0.2046, Val Acc=0.2920, Val F1=0.1848 (Best=0.2062)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/100: 100%|██████████| 140/140 [00:00<00:00, 210.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Train Loss=1.7108, Train Acc=0.3029, Train F1=0.2029, Val Acc=0.2366, Val F1=0.1962 (Best=0.2062)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/100: 100%|██████████| 140/140 [00:00<00:00, 201.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Train Loss=1.7131, Train Acc=0.3147, Train F1=0.2139, Val Acc=0.2277, Val F1=0.1712 (Best=0.2062)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/100: 100%|██████████| 140/140 [00:00<00:00, 206.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Train Loss=1.7079, Train Acc=0.3145, Train F1=0.2204, Val Acc=0.3348, Val F1=0.1995 (Best=0.2062)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/100: 100%|██████████| 140/140 [00:00<00:00, 198.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Train Loss=1.7652, Train Acc=0.2960, Train F1=0.2276, Val Acc=0.3330, Val F1=0.2146 (Best=0.2062)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/100: 100%|██████████| 140/140 [00:00<00:00, 198.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Train Loss=1.7197, Train Acc=0.3205, Train F1=0.2190, Val Acc=0.2777, Val F1=0.1271 (Best=0.2146)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/100: 100%|██████████| 140/140 [00:00<00:00, 198.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Train Loss=1.6946, Train Acc=0.3364, Train F1=0.2401, Val Acc=0.3571, Val F1=0.2275 (Best=0.2146)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/100: 100%|██████████| 140/140 [00:00<00:00, 208.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Train Loss=1.6821, Train Acc=0.3424, Train F1=0.2411, Val Acc=0.3295, Val F1=0.2035 (Best=0.2275)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/100: 100%|██████████| 140/140 [00:00<00:00, 210.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Train Loss=1.6920, Train Acc=0.3384, Train F1=0.2488, Val Acc=0.3420, Val F1=0.1999 (Best=0.2275)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/100: 100%|██████████| 140/140 [00:00<00:00, 201.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Train Loss=1.7008, Train Acc=0.3292, Train F1=0.2356, Val Acc=0.1902, Val F1=0.0776 (Best=0.2275)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/100: 100%|██████████| 140/140 [00:00<00:00, 198.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Train Loss=1.6968, Train Acc=0.3324, Train F1=0.2390, Val Acc=0.3098, Val F1=0.2423 (Best=0.2275)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/100: 100%|██████████| 140/140 [00:00<00:00, 203.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Train Loss=1.6904, Train Acc=0.3391, Train F1=0.2418, Val Acc=0.2705, Val F1=0.1831 (Best=0.2423)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/100: 100%|██████████| 140/140 [00:00<00:00, 208.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Train Loss=1.6885, Train Acc=0.3384, Train F1=0.2438, Val Acc=0.3312, Val F1=0.2632 (Best=0.2423)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/100: 100%|██████████| 140/140 [00:00<00:00, 206.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Train Loss=1.6998, Train Acc=0.3281, Train F1=0.2449, Val Acc=0.3152, Val F1=0.2212 (Best=0.2632)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/100: 100%|██████████| 140/140 [00:00<00:00, 201.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Train Loss=1.6864, Train Acc=0.3413, Train F1=0.2451, Val Acc=0.3045, Val F1=0.1475 (Best=0.2632)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/100: 100%|██████████| 140/140 [00:00<00:00, 209.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Train Loss=1.6911, Train Acc=0.3408, Train F1=0.2492, Val Acc=0.3607, Val F1=0.2550 (Best=0.2632)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/100: 100%|██████████| 140/140 [00:00<00:00, 204.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Train Loss=1.7107, Train Acc=0.3317, Train F1=0.2489, Val Acc=0.3259, Val F1=0.2421 (Best=0.2632)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/100: 100%|██████████| 140/140 [00:00<00:00, 198.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Train Loss=1.6997, Train Acc=0.3375, Train F1=0.2474, Val Acc=0.2705, Val F1=0.1098 (Best=0.2632)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30/100: 100%|██████████| 140/140 [00:00<00:00, 194.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Train Loss=1.6847, Train Acc=0.3415, Train F1=0.2469, Val Acc=0.3482, Val F1=0.2141 (Best=0.2632)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31/100: 100%|██████████| 140/140 [00:00<00:00, 192.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31: Train Loss=1.6962, Train Acc=0.3308, Train F1=0.2364, Val Acc=0.3000, Val F1=0.1583 (Best=0.2632)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32/100: 100%|██████████| 140/140 [00:00<00:00, 198.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32: Train Loss=1.7145, Train Acc=0.3277, Train F1=0.2432, Val Acc=0.3545, Val F1=0.2759 (Best=0.2632)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33/100: 100%|██████████| 140/140 [00:00<00:00, 199.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33: Train Loss=1.6937, Train Acc=0.3308, Train F1=0.2458, Val Acc=0.2045, Val F1=0.1021 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34/100: 100%|██████████| 140/140 [00:00<00:00, 200.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34: Train Loss=1.7024, Train Acc=0.3288, Train F1=0.2416, Val Acc=0.1875, Val F1=0.0672 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35/100: 100%|██████████| 140/140 [00:00<00:00, 200.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35: Train Loss=1.6865, Train Acc=0.3498, Train F1=0.2516, Val Acc=0.3312, Val F1=0.2731 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36/100: 100%|██████████| 140/140 [00:00<00:00, 198.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36: Train Loss=1.6930, Train Acc=0.3379, Train F1=0.2408, Val Acc=0.3125, Val F1=0.1824 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37/100: 100%|██████████| 140/140 [00:00<00:00, 195.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37: Train Loss=1.6999, Train Acc=0.3335, Train F1=0.2450, Val Acc=0.2607, Val F1=0.1721 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38/100: 100%|██████████| 140/140 [00:00<00:00, 201.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38: Train Loss=1.6819, Train Acc=0.3426, Train F1=0.2510, Val Acc=0.3402, Val F1=0.2409 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39/100: 100%|██████████| 140/140 [00:00<00:00, 200.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39: Train Loss=1.6864, Train Acc=0.3509, Train F1=0.2565, Val Acc=0.2250, Val F1=0.1142 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40/100: 100%|██████████| 140/140 [00:00<00:00, 195.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40: Train Loss=1.6961, Train Acc=0.3420, Train F1=0.2496, Val Acc=0.2964, Val F1=0.1509 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 41/100: 100%|██████████| 140/140 [00:00<00:00, 198.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41: Train Loss=1.7145, Train Acc=0.3339, Train F1=0.2488, Val Acc=0.3446, Val F1=0.2017 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 42/100: 100%|██████████| 140/140 [00:00<00:00, 199.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42: Train Loss=1.6949, Train Acc=0.3348, Train F1=0.2411, Val Acc=0.3598, Val F1=0.2407 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43/100: 100%|██████████| 140/140 [00:00<00:00, 199.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43: Train Loss=1.6898, Train Acc=0.3422, Train F1=0.2478, Val Acc=0.2964, Val F1=0.1460 (Best=0.2759)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44/100: 100%|██████████| 140/140 [00:00<00:00, 191.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44: Train Loss=1.6782, Train Acc=0.3451, Train F1=0.2506, Val Acc=0.2196, Val F1=0.1168 (Best=0.2759)\n",
            "Early stopping triggered!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Test e predizioni\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mGraphDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m)\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#test_loader = DataLoader(TestDataset(), batch_size=batch_size)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m evaluate(test_loader, best_model, device, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m~/uni/deep_learning/hackaton/src/loadData.py:52\u001b[0m, in \u001b[0;36mGraphDataset.__init__\u001b[0;34m(self, filename, transform, pre_transform)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pre_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_graphs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraphs_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, transform, pre_transform)\n",
            "File \u001b[0;32m~/uni/deep_learning/hackaton/src/loadData.py:63\u001b[0m, in \u001b[0;36mGraphDataset._count_graphs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count_graphs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 63\u001b[0m         graphs_dicts \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load full JSON array without keeping references\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(graphs_dicts),graphs_dicts\n",
            "File \u001b[0;32m/usr/lib/python3.12/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
            "File \u001b[0;32m/usr/lib/python3.12/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
            "File \u001b[0;32m/usr/lib/python3.12/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gc \n",
        "# Modifica loop principale di training\n",
        "train_datasets_path = [\"datasets/B/train.json.gz\"]\n",
        "\n",
        "for ds in train_datasets_path:\n",
        "\n",
        "\n",
        "    print(\"Generating dataset\")\n",
        "    \n",
        "    full_dataset = GraphDataset(ds, transform=transforms).shuffle()\n",
        "    #full_dataset = TestDataset()\n",
        "    print(\"Splitting dataset\")\n",
        "    train_set, val_set = split_dataset(full_dataset, 0.2, seed)\n",
        "    \n",
        "\n",
        "    model_kwargs = {\"gnn_type\": gnn_type, \"num_classes\":num_classes, \"num_layer\": num_layer, \"emb_dim\": emb_dim, \"drop_ratio\": drop_ratio, \"device\":device, \"residual\": residual, \"pooling\": pooling }\n",
        "    model = create_gnn_model(**model_kwargs)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    \n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    best_model = train(model=model, \n",
        "                       train_dataset=train_set, \n",
        "                       val_dataset=val_set, \n",
        "                       criterion=criterion, \n",
        "                       optimizer=optimizer,  \n",
        "                       device=device,\n",
        "                       num_epochs=epochs, \n",
        "                       patience=patience, \n",
        "                       batch_size=batch_size)\n",
        "\n",
        "    del val_set, train_set, full_dataset, criterion\n",
        "    gc.collect()\n",
        "\n",
        "    # Test e predizioni\n",
        "    test_loader = DataLoader(GraphDataset(ds.replace(\"train\", \"test\"), transform=transforms), batch_size=32)\n",
        "    #test_loader = DataLoader(TestDataset(), batch_size=batch_size)\n",
        "    predictions = evaluate(test_loader, best_model, device, False)\n",
        "    save_predictions(predictions=predictions, test_path=ds.replace(\"train\", \"test\"))\n",
        "    del test_loader, best_model, predictions\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "machine_learning_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 108.457758,
      "end_time": "2025-05-21T16:25:04.482169",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-05-21T16:23:16.024411",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
