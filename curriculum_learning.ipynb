{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6228bc1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6228bc1c",
        "outputId": "21efda41-d41b-4a8b-fb84-92261046b64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (6.1.1)\n",
            "Requirement already satisfied: pyparsing in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (2024.12.14)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        " !pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bdWYrgjeZerj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdWYrgjeZerj",
        "outputId": "03a79ed5-17fa-4c42-c2ff-585d03588990"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"from google.colab import drive\n",
        "drive.mount('/content/drive')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "hbCBqNgfICMW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbCBqNgfICMW",
        "outputId": "0ab5c89c-2f13-4473-b582-9f5bd22e9a33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%cd /content/drive/MyDrive/DeepLearning/hackaton'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"%cd /content/drive/MyDrive/DeepLearning/hackaton\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "817b1078",
      "metadata": {
        "id": "817b1078"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# PyG\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Vostri moduli\n",
        "from src.loadData    import GraphDataset\n",
        "from src.models      import GNN\n",
        "from src.transforms import EdgeDropout, NodeDropout, Compose, GraphMixUp\n",
        "from src.losses import (\n",
        "    GeneralizedCELoss,\n",
        "    SymmetricCELoss,\n",
        "    estimate_transition_matrix,\n",
        "    ForwardCorrectionLoss,\n",
        "    BootstrappingLoss\n",
        ")\n",
        "from src.pretraining import GraphCLTrainer, add_zeros\n",
        "from src.divide_mix_def import DivideMixTrainer\n",
        "\n",
        "# Fissa seed\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0a9c70d7",
      "metadata": {
        "id": "0a9c70d7"
      },
      "outputs": [],
      "source": [
        "def add_zeros(data):\n",
        "    if not hasattr(data, 'x') or data.x is None:\n",
        "        data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3622cfa1",
      "metadata": {
        "id": "3622cfa1"
      },
      "outputs": [],
      "source": [
        "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "        total += data.y.size(0)\n",
        "\n",
        "    # Save checkpoints if required\n",
        "    if save_checkpoints:\n",
        "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
        "        torch.save(model.state_dict(), checkpoint_file)\n",
        "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
        "\n",
        "    print(f\"Train loss/acc: {total_loss / len(data_loader):.4f}/{correct / total:.4f}\")\n",
        "    return total_loss / len(data_loader),  correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6139b912",
      "metadata": {
        "id": "6139b912"
      },
      "outputs": [],
      "source": [
        "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions = []\n",
        "    total_loss = 0\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "\n",
        "            if calculate_accuracy:\n",
        "                correct += (pred == data.y).sum().item()\n",
        "                total += data.y.size(0)\n",
        "                total_loss += criterion(output, data.y).item()\n",
        "            else:\n",
        "                predictions.extend(pred.cpu().numpy())\n",
        "    if calculate_accuracy:\n",
        "        accuracy = correct / total\n",
        "        return  total_loss / len(data_loader),accuracy\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fbdbd871",
      "metadata": {
        "id": "fbdbd871"
      },
      "outputs": [],
      "source": [
        "def save_predictions(predictions, test_path):\n",
        "    script_dir = os.getcwd()\n",
        "    submission_folder = os.path.join(script_dir, \"submission\")\n",
        "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
        "\n",
        "    os.makedirs(submission_folder, exist_ok=True)\n",
        "\n",
        "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
        "\n",
        "    test_graph_ids = list(range(len(predictions)))\n",
        "    output_df = pd.DataFrame({\n",
        "        \"id\": test_graph_ids,\n",
        "        \"pred\": predictions\n",
        "    })\n",
        "\n",
        "    output_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Predictions saved to {output_csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fc3d24da",
      "metadata": {
        "id": "fc3d24da"
      },
      "outputs": [],
      "source": [
        "def plot_training_progress(train_losses, train_accuracies, save_plot, output_dir):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss per Epoch')\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training Accuracy per Epoch')\n",
        "\n",
        "    if(save_plot):\n",
        "        # Save plots in the current directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cMtQXyuWQol",
      "metadata": {
        "id": "8cMtQXyuWQol"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "device                =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Dataset & I/O\n",
        "num_classes           = 6\n",
        "batch_size            = 32\n",
        "\n",
        "# Modello\n",
        "gnn_type              = 'gin-virtual'    # scegli fra 'gin','gin-virtual','gcn','gcn-virtual'\n",
        "num_layer             = 5\n",
        "emb_dim               = 256\n",
        "drop_ratio            = 0.4\n",
        "lr                    = 5e-4\n",
        "weight_decay          = 5e-4\n",
        "epochs                = 50\n",
        "\n",
        "num_epochs_initial = 5\n",
        "num_epochs_remaining = 50\n",
        "tau = 0.5\n",
        "patience = 10\n",
        "best_val_acc = 0.0\n",
        "trigger_times = 0\n",
        "\n",
        "\n",
        "# Trasform\n",
        "edge_p  = 0.2   # EdgeDropout\n",
        "node_p  = 0.2   # NodeDropout\n",
        "\n",
        "transforms = Compose([\n",
        "    EdgeDropout(p=edge_p),\n",
        "    add_zeros,\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iIG-ybDv1F_6",
      "metadata": {
        "id": "iIG-ybDv1F_6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "41679bf6",
      "metadata": {
        "id": "41679bf6"
      },
      "source": [
        "# Co-Teaching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "502f55df",
      "metadata": {
        "id": "502f55df"
      },
      "outputs": [],
      "source": [
        "def create_gnn_model(gnn_type, num_classes, num_layer, emb_dim, drop_ratio, device):\n",
        "    kwargs = {\n",
        "        'gnn_type': gnn_type.replace(\"-virtual\", \"\"),\n",
        "        'num_class': num_classes,\n",
        "        'num_layer': num_layer,\n",
        "        'emb_dim': emb_dim,\n",
        "        'drop_ratio': drop_ratio,\n",
        "        'virtual_node': \"virtual\" in gnn_type,\n",
        "        'residual': True\n",
        "        }\n",
        "\n",
        "    model = GNN(**kwargs).to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_optimizer_and_scheduler(model, lr=1e-3, weight_decay=1e-4, t_max=50, eta_min=1e-5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
        "    return optimizer, scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7161c891",
      "metadata": {
        "id": "7161c891"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import torch\n",
        "\n",
        "class CurriculumTrainer:\n",
        "    def __init__(self, model, train_dataset, val_dataset, criterion, \n",
        "                 optimizer, scheduler, device, num_classes):\n",
        "        self.model = model\n",
        "        self.full_train_dataset = train_dataset  # Keep original dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.best_val_acc = 0\n",
        "        self.best_model = None\n",
        "        \n",
        "    def _create_filtered_loader(self, model, loader, keep_frac=0.85):\n",
        "        \"\"\"Create new DataLoader with filtered samples\"\"\"\n",
        "        model.eval()\n",
        "        indices = []\n",
        "        losses = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                batch = batch.to(self.device)\n",
        "                outputs = model(batch)\n",
        "                loss = self.criterion(outputs, batch.y, reduction='none')\n",
        "                losses.append(loss.cpu())\n",
        "                indices.extend(batch.idx.tolist())  # Assuming each graph has an idx\n",
        "        \n",
        "        losses = torch.cat(losses)\n",
        "        threshold = losses.quantile(keep_frac)\n",
        "        clean_indices = [idx for i, idx in enumerate(indices) if losses[i] <= threshold]\n",
        "        \n",
        "        return DataLoader(\n",
        "            Subset(self.full_train_dataset, clean_indices),\n",
        "            batch_size=loader.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=loader.num_workers\n",
        "        )\n",
        "\n",
        "    def train(self, num_epochs=100, patience=10, \n",
        "              curriculum_epoch=5, forward_corr_epoch=10):\n",
        "        \n",
        "        # Initial loader uses full dataset\n",
        "        current_train_loader = DataLoader(\n",
        "            self.full_train_dataset,\n",
        "            batch_size=32,\n",
        "            shuffle=True,\n",
        "            num_workers=4\n",
        "        )\n",
        "        \n",
        "        for epoch in range(num_epochs):\n",
        "            self.model.train()\n",
        "            total_loss, total_correct, total_samples = 0, 0, 0\n",
        "\n",
        "            for batch in tqdm(current_train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                batch = batch.to(self.device)\n",
        "                \n",
        "                # Forward pass\n",
        "                outputs = self.model(batch)\n",
        "                loss = self.criterion(outputs, batch.y)\n",
        "                \n",
        "                # Backward pass\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                # Metrics\n",
        "                total_loss += loss.item() * batch.y.size(0)\n",
        "                total_correct += (outputs.argmax(1) == batch.y).sum().item()\n",
        "                total_samples += batch.y.size(0)\n",
        "\n",
        "            # Validation\n",
        "            val_acc = self.validate()\n",
        "            avg_loss = total_loss / total_samples\n",
        "            train_acc = total_correct / total_samples\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_acc > self.best_val_acc:\n",
        "                self.best_val_acc = val_acc\n",
        "                self.best_model = deepcopy(self.model.state_dict())\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(\"Early stopping triggered!\")\n",
        "                    break\n",
        "\n",
        "            # Curriculum learning (create new loader)\n",
        "            if epoch == curriculum_epoch:\n",
        "                print(\"Activating curriculum learning...\")\n",
        "                current_train_loader = self._create_filtered_loader(\n",
        "                    self.model, current_train_loader\n",
        "                )\n",
        "\n",
        "            # Forward correction\n",
        "            if epoch == forward_corr_epoch:\n",
        "                print(\"Activating forward correction...\")\n",
        "                self.criterion = self._estimate_forward_correction(current_train_loader)\n",
        "\n",
        "            self.scheduler.step()\n",
        "\n",
        "        # Load best model\n",
        "        self.model.load_state_dict(self.best_model)\n",
        "        return self.model\n",
        "\n",
        "    def _estimate_forward_correction(self, loader):\n",
        "        \"\"\"Estimate transition matrix and return new loss\"\"\"\n",
        "        self.model.eval()\n",
        "        conf_matrix = torch.zeros(self.num_classes, self.num_classes)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                batch = batch.to(self.device)\n",
        "                outputs = self.model(batch)\n",
        "                preds = outputs.argmax(1)\n",
        "                for p, y in zip(preds, batch.y):\n",
        "                    conf_matrix[y, p] += 1\n",
        "\n",
        "        # Normalize and smooth\n",
        "        T = conf_matrix / (conf_matrix.sum(1, keepdim=True) + 1e-6)\n",
        "        T = (T + torch.eye(self.num_classes, device=T.device)*0.1) / 1.1  # Smoothing\n",
        "        \n",
        "        return ForwardCorrectionLoss(T).to(self.device)\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        val_loader = DataLoader(self.val_dataset, batch_size=32, shuffle=False)\n",
        "        correct, total = 0, 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(self.device)\n",
        "                outputs = self.model(batch)\n",
        "                preds = outputs.argmax(1)\n",
        "                correct += (preds == batch.y).sum().item()\n",
        "                total += batch.y.size(0)\n",
        "                \n",
        "        return correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c9b1c83c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_dataset(dataset: GraphDataset, val_ratio=0.1, seed=42):\n",
        "    labels = torch.tensor([data.y.item() for data in dataset])\n",
        "    indices = list(range(len(dataset)))\n",
        "\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=val_ratio,\n",
        "        stratify=labels,\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    train_subset = []\n",
        "    val_subset = []\n",
        "\n",
        "    for new_idx, original_idx in enumerate(train_idx):\n",
        "        data = dataset[original_idx]\n",
        "        data.idx = new_idx  # Normalize index\n",
        "        train_subset.append(data)\n",
        "\n",
        "    for new_idx, original_idx in enumerate(val_idx):\n",
        "        data = dataset[original_idx]\n",
        "        data.idx = new_idx  # Normalize index\n",
        "        val_subset.append(data)\n",
        "\n",
        "    return train_subset, val_subset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "64f60cfb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating dataset\n",
            "Splitting dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 257/257 [00:08<00:00, 30.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss=9.2879, Train Acc=0.3000, Val Acc=0.3487\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 257/257 [00:08<00:00, 30.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss=8.4548, Train Acc=0.3684, Val Acc=0.3750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 257/257 [00:08<00:00, 30.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss=8.0364, Train Acc=0.3940, Val Acc=0.3755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 257/257 [00:08<00:00, 30.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss=7.6564, Train Acc=0.4190, Val Acc=0.4008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 257/257 [00:08<00:00, 31.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss=7.3039, Train Acc=0.4475, Val Acc=0.4694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 257/257 [00:08<00:00, 30.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss=7.0688, Train Acc=0.4615, Val Acc=0.2738\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 257/257 [00:08<00:00, 30.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss=6.8667, Train Acc=0.4785, Val Acc=0.4723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 257/257 [00:08<00:00, 30.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss=6.6115, Train Acc=0.4977, Val Acc=0.3536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 257/257 [00:08<00:00, 30.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss=6.4873, Train Acc=0.5039, Val Acc=0.4027\n",
            "Activating curriculum learning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 219/219 [00:07<00:00, 31.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss=6.4127, Train Acc=0.5004, Val Acc=0.4105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|██████████| 219/219 [00:07<00:00, 31.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss=6.2984, Train Acc=0.5149, Val Acc=0.4737\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|██████████| 219/219 [00:07<00:00, 31.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss=6.1505, Train Acc=0.5240, Val Acc=0.3658\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|██████████| 219/219 [00:07<00:00, 29.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss=6.1227, Train Acc=0.5243, Val Acc=0.4611\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|██████████| 219/219 [00:07<00:00, 30.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss=5.9858, Train Acc=0.5372, Val Acc=0.5545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|██████████| 219/219 [00:07<00:00, 29.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss=5.9783, Train Acc=0.5371, Val Acc=0.5676\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|██████████| 219/219 [00:06<00:00, 31.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss=5.7578, Train Acc=0.5542, Val Acc=0.3463\n",
            "Activating forward correction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|██████████| 219/219 [00:07<00:00, 30.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss=1.6695, Train Acc=0.4700, Val Acc=0.3502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|██████████| 219/219 [00:07<00:00, 30.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss=1.6078, Train Acc=0.4601, Val Acc=0.4523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|██████████| 219/219 [00:07<00:00, 30.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss=1.5884, Train Acc=0.4805, Val Acc=0.4859\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|██████████| 219/219 [00:07<00:00, 30.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss=1.6026, Train Acc=0.4629, Val Acc=0.4509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|██████████| 219/219 [00:07<00:00, 31.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss=1.5833, Train Acc=0.4738, Val Acc=0.5389\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|██████████| 219/219 [00:07<00:00, 30.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss=1.5756, Train Acc=0.4795, Val Acc=0.4139\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|██████████| 219/219 [00:07<00:00, 30.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss=1.5812, Train Acc=0.4715, Val Acc=0.4912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|██████████| 219/219 [00:07<00:00, 30.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss=1.5793, Train Acc=0.4734, Val Acc=0.4236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|██████████| 219/219 [00:06<00:00, 31.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss=1.5821, Train Acc=0.4712, Val Acc=0.4640\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|██████████| 219/219 [00:07<00:00, 30.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss=1.5665, Train Acc=0.4790, Val Acc=0.5131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|██████████| 219/219 [00:07<00:00, 30.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss=1.5681, Train Acc=0.4735, Val Acc=0.3624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|██████████| 219/219 [00:07<00:00, 30.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss=1.5678, Train Acc=0.4757, Val Acc=0.4689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|██████████| 219/219 [00:07<00:00, 30.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss=1.5748, Train Acc=0.4765, Val Acc=0.5063\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|██████████| 219/219 [00:06<00:00, 31.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss=1.5600, Train Acc=0.4824, Val Acc=0.5151\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31: 100%|██████████| 219/219 [00:06<00:00, 31.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31: Loss=1.5647, Train Acc=0.4794, Val Acc=0.5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32: 100%|██████████| 219/219 [00:07<00:00, 30.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32: Loss=1.5538, Train Acc=0.4838, Val Acc=0.5457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33: 100%|██████████| 219/219 [00:07<00:00, 31.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33: Loss=1.5518, Train Acc=0.4910, Val Acc=0.4567\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34: 100%|██████████| 219/219 [00:07<00:00, 29.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34: Loss=1.5538, Train Acc=0.4861, Val Acc=0.5302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35: 100%|██████████| 219/219 [00:07<00:00, 30.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35: Loss=1.5525, Train Acc=0.4835, Val Acc=0.4981\n",
            "Early stopping triggered!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.40batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to /home/flaviolinux/uni/deep_learning/hackaton/submission/testset_D.csv\n"
          ]
        }
      ],
      "source": [
        "import gc \n",
        "# Modifica loop principale di training\n",
        "train_datasets_path = [\"datasets/D/train.json.gz\"]\n",
        "\n",
        "for ds in train_datasets_path:\n",
        "\n",
        "\n",
        "    print(\"Generating dataset\")\n",
        "    try:\n",
        "        full_dataset = GraphDataset(ds, transform=transforms).shuffle()\n",
        "    except:\n",
        "        del full_dataset\n",
        "        gc.collect()\n",
        "    \n",
        "    print(\"Splitting dataset\")\n",
        "    train_set, val_set = split_dataset(full_dataset, 0.2, 42)\n",
        "\n",
        "    model = create_gnn_model(\"gin-virtual\", num_classes=num_classes, num_layer=num_layer, emb_dim=emb_dim, drop_ratio=drop_ratio, device=device)\n",
        "    optimizer, scheduler = create_optimizer_and_scheduler(model, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    criterion = SymmetricCELoss(alpha=0.4, beta=0.6).to(device)\n",
        "    \n",
        "    trainer = CurriculumTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_set,  \n",
        "        val_dataset=val_set,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        num_classes=6\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    best_model = trainer.train(\n",
        "        num_epochs=100,\n",
        "        patience=20,\n",
        "        curriculum_epoch=8,\n",
        "        forward_corr_epoch=15\n",
        "    )\n",
        "\n",
        "    del val_set, train_set, model, trainer, full_dataset\n",
        "    gc.collect()\n",
        "\n",
        "    # Test e predizioni\n",
        "    test_loader = DataLoader(GraphDataset(ds.replace(\"train\", \"test\"), transform=transforms), batch_size=32)\n",
        "    predictions = evaluate(test_loader, best_model, device, False)\n",
        "    save_predictions(predictions=predictions, test_path=ds.replace(\"train\", \"test\"))\n",
        "\n",
        "    del test_loader, criterion, optimizer, scheduler, best_model, predictions\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "59b85f84",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "machine_learning_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 108.457758,
      "end_time": "2025-05-21T16:25:04.482169",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-05-21T16:23:16.024411",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
