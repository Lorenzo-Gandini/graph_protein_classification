{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6228bc1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6228bc1c",
        "outputId": "fb845e61-cc5e-4839-9774-9f56dc526421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (6.1.1)\n",
            "Requirement already satisfied: pyparsing in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/flaviolinux/machine_learning_env/lib/python3.12/site-packages (from requests->torch_geometric) (2024.12.14)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bdWYrgjeZerj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdWYrgjeZerj",
        "outputId": "a1df1bdc-ad74-410f-9378-058c587a5db8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"from google.colab import drive\n",
        "drive.mount('/content/drive')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "817b1078",
      "metadata": {
        "id": "817b1078"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "# PyG\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Vostri moduli\n",
        "from src.loadData    import (GraphDataset, TestDataset)\n",
        "from src.models      import GNN\n",
        "from src.transforms import EdgeDropout, NodeDropout, Compose\n",
        "from src.losses import (\n",
        "    GeneralizedCELoss,\n",
        "    SymmetricCELoss,\n",
        "    estimate_transition_matrix,\n",
        "    ForwardCorrectionLoss,\n",
        "    BootstrappingLoss\n",
        ")\n",
        "from src.divide_mix_def import DivideMixTrainer\n",
        "\n",
        "# Fissa seed\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0a9c70d7",
      "metadata": {
        "id": "0a9c70d7"
      },
      "outputs": [],
      "source": [
        "def add_zeros(data):\n",
        "    if not hasattr(data, 'x') or data.x is None:\n",
        "        data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3622cfa1",
      "metadata": {
        "id": "3622cfa1"
      },
      "outputs": [],
      "source": [
        "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
        "    \n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "        total += data.y.size(0)\n",
        "\n",
        "    # Save checkpoints if required\n",
        "    if save_checkpoints:\n",
        "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
        "        torch.save(model.state_dict(), checkpoint_file)\n",
        "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
        "\n",
        "    print(f\"Train loss/acc: {total_loss / len(data_loader):.4f}/{correct / total:.4f}\")\n",
        "    return total_loss / len(data_loader),  correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6139b912",
      "metadata": {
        "id": "6139b912"
      },
      "outputs": [],
      "source": [
        "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions = []\n",
        "    total_loss = 0\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "\n",
        "            if calculate_accuracy:\n",
        "                correct += (pred == data.y).sum().item()\n",
        "                total += data.y.size(0)\n",
        "                total_loss += criterion(output, data.y).item()\n",
        "            else:\n",
        "                predictions.extend(pred.cpu().numpy())\n",
        "\n",
        "    \n",
        "    if calculate_accuracy:\n",
        "        accuracy = correct / total\n",
        "        return  total_loss / len(data_loader),accuracy\n",
        "        print(f\"Test loss/acc {total_loss / len(data_loader):.4f} / {correct / total:.4f}\")\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fbdbd871",
      "metadata": {
        "id": "fbdbd871"
      },
      "outputs": [],
      "source": [
        "def save_predictions(predictions, test_path):\n",
        "    script_dir = os.getcwd()\n",
        "    submission_folder = os.path.join(script_dir, \"submission\")\n",
        "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
        "\n",
        "    os.makedirs(submission_folder, exist_ok=True)\n",
        "\n",
        "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
        "\n",
        "    test_graph_ids = list(range(len(predictions)))\n",
        "    output_df = pd.DataFrame({\n",
        "        \"id\": test_graph_ids,\n",
        "        \"pred\": predictions\n",
        "    })\n",
        "\n",
        "    output_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Predictions saved to {output_csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fc3d24da",
      "metadata": {
        "id": "fc3d24da"
      },
      "outputs": [],
      "source": [
        "def plot_training_progress(train_losses, train_accuracies, save_plot, output_dir):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss per Epoch')\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training Accuracy per Epoch')\n",
        "\n",
        "    if(save_plot):\n",
        "        # Save plots in the current directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8cMtQXyuWQol",
      "metadata": {
        "id": "8cMtQXyuWQol"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "device                =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Modello\n",
        "gnn_type              = 'gin-virtual'   \n",
        "num_layer             = 3\n",
        "emb_dim               = 300\n",
        "drop_ratio            = 0.7\n",
        "\n",
        "edge_p  = 0.2   # frazione di bordi da droppare\n",
        "node_p = 0.2\n",
        "\n",
        "lr                    = 0.0005\n",
        "epochs                = 50\n",
        "weight_decay          = 5e-4\n",
        "t_max                 = 50\n",
        "eta_min               = 1e-5\n",
        "num_classes           = 6\n",
        "batch_size            = 32 \n",
        "patience              = 12\n",
        "warmup_epochs         = 10\n",
        "lambda_u              = 1\n",
        "T                     = 0.6\n",
        "alpha                 = 6\n",
        "p_threshold           = 0.8\n",
        "\n",
        "transforms = Compose([\n",
        "    EdgeDropout(p=edge_p),\n",
        "    NodeDropout(p=node_p),\n",
        "    add_zeros,\n",
        "])\n",
        "\n",
        "# Epoch 49: Train Loss=0.3052, Train Acc=0.9117, Val Acc=0.6953 DATASET C"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41679bf6",
      "metadata": {},
      "source": [
        "# Divide-Mix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "502f55df",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_gnn_model(gnn_type, num_classes, num_layer, emb_dim, drop_ratio, device, residual):\n",
        "    kwargs = {\n",
        "        'gnn_type': gnn_type.replace(\"-virtual\", \"\"),\n",
        "        'num_class': num_classes,\n",
        "        'num_layer': num_layer,\n",
        "        'emb_dim': emb_dim,\n",
        "        'drop_ratio': drop_ratio,\n",
        "        'virtual_node': \"virtual\" in gnn_type,\n",
        "        'residual': residual\n",
        "        }\n",
        "\n",
        "    model = GNN(**kwargs).to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_optimizer_and_scheduler(model, lr=lr, weight_decay=weight_decay, t_max=t_max, eta_min=eta_min):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
        "    return optimizer, scheduler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b7538579",
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_dataset(dataset: GraphDataset, val_ratio=0.1, seed=42):\n",
        "    labels = torch.tensor([data.y.item() for data in dataset])\n",
        "    indices = list(range(len(dataset)))\n",
        "\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=val_ratio,\n",
        "        stratify=labels,\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    train_subset = []\n",
        "    val_subset = []\n",
        "\n",
        "    for new_idx, original_idx in enumerate(train_idx):\n",
        "        data = dataset[original_idx]\n",
        "        data.idx = new_idx  # Normalize index\n",
        "        train_subset.append(data)\n",
        "\n",
        "    for new_idx, original_idx in enumerate(val_idx):\n",
        "        data = dataset[original_idx]\n",
        "        data.idx = new_idx  # Normalize index\n",
        "        val_subset.append(data)\n",
        "\n",
        "    return train_subset, val_subset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4696ca96",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating dataset\n",
            "Splitting dataset\n",
            "Warmup phase (10 Epochs)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:04<00:00, 28.08it/s]\n",
            "100%|██████████| 140/140 [00:04<00:00, 29.84it/s]\n",
            "100%|██████████| 140/140 [00:04<00:00, 30.06it/s]\n",
            "100%|██████████| 140/140 [00:04<00:00, 28.58it/s]\n",
            "100%|██████████| 140/140 [00:04<00:00, 28.48it/s]\n",
            "100%|██████████| 140/140 [00:04<00:00, 29.31it/s]\n",
            "100%|██████████| 140/140 [00:04<00:00, 29.85it/s]\n",
            "100%|██████████| 140/140 [00:04<00:00, 29.28it/s]\n",
            "100%|██████████| 140/140 [00:04<00:00, 30.27it/s]\n",
            "100%|██████████| 140/140 [00:04<00:00, 29.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End of Warmup Stage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 | Sup=1.6524 | Unsup=1.3841 | TrainAcc=0.3636 | ValAcc=0.3875 (Best=0.0000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/50 | Sup=1.6422 | Unsup=1.3744 | TrainAcc=0.3743 | ValAcc=0.3625 (Best=0.3875)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/50 | Sup=1.6355 | Unsup=1.3510 | TrainAcc=0.3906 | ValAcc=0.4000 (Best=0.3875)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/50 | Sup=1.6213 | Unsup=1.3028 | TrainAcc=0.3350 | ValAcc=0.3304 (Best=0.4000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/50 | Sup=1.6154 | Unsup=1.2712 | TrainAcc=0.3710 | ValAcc=0.3857 (Best=0.4000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/50 | Sup=1.5934 | Unsup=1.2395 | TrainAcc=0.3609 | ValAcc=0.3625 (Best=0.4000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/50 | Sup=1.5912 | Unsup=1.1802 | TrainAcc=0.4025 | ValAcc=0.4054 (Best=0.4000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/50 | Sup=1.5852 | Unsup=1.1507 | TrainAcc=0.4129 | ValAcc=0.4027 (Best=0.4054)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/50 | Sup=1.5777 | Unsup=1.1274 | TrainAcc=0.4116 | ValAcc=0.4045 (Best=0.4054)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/50 | Sup=1.5659 | Unsup=1.0995 | TrainAcc=0.3717 | ValAcc=0.3714 (Best=0.4054)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/50 | Sup=1.5669 | Unsup=1.0756 | TrainAcc=0.4179 | ValAcc=0.4223 (Best=0.4054)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50 | Sup=1.5435 | Unsup=1.0244 | TrainAcc=0.3955 | ValAcc=0.3929 (Best=0.4223)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/50 | Sup=1.5423 | Unsup=1.0004 | TrainAcc=0.3991 | ValAcc=0.3929 (Best=0.4223)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/50 | Sup=1.5352 | Unsup=0.9763 | TrainAcc=0.4087 | ValAcc=0.4196 (Best=0.4223)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/50 | Sup=1.5416 | Unsup=0.9499 | TrainAcc=0.4373 | ValAcc=0.4259 (Best=0.4223)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/50 | Sup=1.5160 | Unsup=0.9282 | TrainAcc=0.4083 | ValAcc=0.4107 (Best=0.4259)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/50 | Sup=1.5187 | Unsup=0.8849 | TrainAcc=0.3908 | ValAcc=0.3857 (Best=0.4259)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/50 | Sup=1.4989 | Unsup=0.8975 | TrainAcc=0.4359 | ValAcc=0.4366 (Best=0.4259)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/50 | Sup=1.5066 | Unsup=0.8606 | TrainAcc=0.4382 | ValAcc=0.4232 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/50 | Sup=1.4910 | Unsup=0.8380 | TrainAcc=0.4109 | ValAcc=0.4143 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/50 | Sup=1.4745 | Unsup=0.8173 | TrainAcc=0.4319 | ValAcc=0.4196 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/50 | Sup=1.4534 | Unsup=0.7635 | TrainAcc=0.4263 | ValAcc=0.4009 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/50 | Sup=1.4499 | Unsup=0.7640 | TrainAcc=0.4167 | ValAcc=0.3911 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/50 | Sup=1.4442 | Unsup=0.7436 | TrainAcc=0.4440 | ValAcc=0.4268 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/50 | Sup=1.4302 | Unsup=0.7270 | TrainAcc=0.4176 | ValAcc=0.4018 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/50 | Sup=1.4242 | Unsup=0.7271 | TrainAcc=0.4551 | ValAcc=0.4357 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/50 | Sup=1.4204 | Unsup=0.6957 | TrainAcc=0.4368 | ValAcc=0.4125 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:10<00:00, 13.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/50 | Sup=1.4117 | Unsup=0.6866 | TrainAcc=0.4545 | ValAcc=0.4295 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/50 | Sup=1.3948 | Unsup=0.6610 | TrainAcc=0.4609 | ValAcc=0.4348 (Best=0.4366)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:09<00:00, 14.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/50 | Sup=1.3987 | Unsup=0.6494 | TrainAcc=0.4518 | ValAcc=0.4321 (Best=0.4366)\n",
            "Early stopping triggered!\n",
            "Evaluating model 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 24/49 [00:02<00:02, 11.90it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(GraphDataset(ds\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m), transform\u001b[38;5;241m=\u001b[39mtransforms), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#test_loader = DataLoader(TestDataset(), batch_size=batch_size)\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mDivideMixTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m save_predictions(predictions\u001b[38;5;241m=\u001b[39mpredictions, test_path\u001b[38;5;241m=\u001b[39mds\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m test_loader, criterion, best_model, predictions\n",
            "File \u001b[0;32m~/uni/deep_learning/hackaton/src/divide_mix_def.py:281\u001b[0m, in \u001b[0;36mpredict_ensemble\u001b[0;34m(models, dataloader, device)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 281\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    282\u001b[0m logits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "File \u001b[0;32m~/machine_learning_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "File \u001b[0;32m~/machine_learning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
            "File \u001b[0;32m~/machine_learning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/machine_learning_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/machine_learning_env/lib/python3.12/site-packages/torch_geometric/data/dataset.py:291\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 291\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/uni/deep_learning/hackaton/src/loadData.py:59\u001b[0m, in \u001b[0;36mGraphDataset.get\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdictToGraphObject\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraphs_dicts\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/uni/deep_learning/hackaton/src/loadData.py:68\u001b[0m, in \u001b[0;36mdictToGraphObject\u001b[0;34m(graph_dict, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdictToGraphObject\u001b[39m(graph_dict, idx):\n\u001b[1;32m     67\u001b[0m     edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(graph_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 68\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medge_attr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m graph_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     num_nodes \u001b[38;5;241m=\u001b[39m graph_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     70\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(graph_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mif\u001b[39;00m graph_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gc \n",
        "# Modifica loop principale di training\n",
        "train_datasets_path = [\"datasets/B/train.json.gz\"]\n",
        "\n",
        "for ds in train_datasets_path:\n",
        "\n",
        "\n",
        "    print(\"Generating dataset\")\n",
        "    \n",
        "    full_dataset = GraphDataset(ds, transform=transforms).shuffle()\n",
        "    #full_dataset = TestDataset()\n",
        "    print(\"Splitting dataset\")\n",
        "    train_set, val_set = split_dataset(full_dataset, 0.2, 42)\n",
        "    \n",
        "\n",
        "    model_kwargs = {\"gnn_type\": \"gin-virtual\", \"num_classes\":num_classes, \"num_layer\": num_layer, \"emb_dim\": emb_dim, \"drop_ratio\": drop_ratio, \"device\":device, \"residual\": False }\n",
        "    optim_kwargs = {\"lr\": lr, \"weight_decay\": weight_decay, \"t_max\": t_max, \"eta_min\": eta_min}\n",
        "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    trainer = DivideMixTrainer(\n",
        "        model_creator=create_gnn_model,\n",
        "        model_kwargs=model_kwargs, \n",
        "        criterion=criterion,\n",
        "        optim_sched_creator=create_optimizer_and_scheduler,\n",
        "        optim_sched_kwargs=optim_kwargs,\n",
        "        device=device,\n",
        "        num_models=2,\n",
        "        warmup_epochs=warmup_epochs,\n",
        "        lambda_u=lambda_u, \n",
        "        T = T,\n",
        "        alpha = alpha,\n",
        "        p_threshold=p_threshold,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    best_model = trainer.train_full_pipeline(\n",
        "        train_dataset=train_set,\n",
        "        val_dataset=val_set,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        patience = patience\n",
        "    )\n",
        "\n",
        "    del val_set, train_set, trainer, full_dataset\n",
        "    gc.collect()\n",
        "\n",
        "    # Test e predizioni\n",
        "    test_loader = DataLoader(GraphDataset(ds.replace(\"train\", \"test\"), transform=transforms), batch_size=32)\n",
        "    #test_loader = DataLoader(TestDataset(), batch_size=batch_size)\n",
        "    predictions = DivideMixTrainer.predict_ensemble(best_model, test_loader, device)\n",
        "    save_predictions(predictions=predictions, test_path=ds.replace(\"train\", \"test\"))\n",
        "    del test_loader, criterion, best_model, predictions\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "machine_learning_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 108.457758,
      "end_time": "2025-05-21T16:25:04.482169",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-05-21T16:23:16.024411",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
